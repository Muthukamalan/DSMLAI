{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'> Character level GPT</h1>\n",
    "<div align='center'><description>Inspired by karapathy's NanoGPT Charlevel Encoder ðŸš€</description></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muthu\\miniconda3\\envs\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc \n",
    "gc.collect()\n",
    "\n",
    "import torch\n",
    "import lightning as pl\n",
    "from torchinfo import summary\n",
    "\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "from functorch.compile import compiled_function, draw_graph\n",
    "from lightning.pytorch.profilers import PyTorchProfiler\n",
    "from lightning.pytorch.callbacks import (\n",
    "    DeviceStatsMonitor,\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    "    ModelPruning,\n",
    ")\n",
    "from lightning.pytorch.callbacks.progress import TQDMProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import CONFIG\n",
    "from model import NanoGPT\n",
    "from dataum import LitAuthorData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 270498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "270498"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32=True\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "torch.cuda.amp.autocast(enabled=True, dtype=torch.float16)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "torch.set_default_device(device=device)\n",
    "torch.cuda.empty_cache()\n",
    "pl.seed_everything(270498);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loggers\n",
    "logger: pl_loggers.TensorBoardLogger = pl_loggers.TensorBoardLogger(\n",
    "    save_dir=\"logs/\", name=\"nanogpt\", log_graph=True\n",
    ")\n",
    "\n",
    "\n",
    "## CallBacks\n",
    "call_backs = [\n",
    "    TQDMProgressBar(refresh_rate=10),\n",
    "    ModelCheckpoint(\n",
    "        monitor=\"val/loss\",\n",
    "        dirpath=os.path.join(\"logs\", \"chkpoints\"),\n",
    "        filename=\"{epoch:02d}\",\n",
    "        save_top_k=1,\n",
    "    ),\n",
    "    DeviceStatsMonitor(cpu_stats=True),\n",
    "    EarlyStopping(monitor=\"val/loss\",mode='min'),\n",
    "    LearningRateMonitor(logging_interval=\"step\"),\n",
    "]\n",
    "\n",
    "# Profiler\n",
    "perf_dir = os.path.join(os.getcwd(), \"logs\", \"profiler\")\n",
    "perf_profiler = PyTorchProfiler(\n",
    "    dirpath=perf_dir,\n",
    "    filename=\"perf_logs_pytorch\",\n",
    "    group_by_input_shapes=True,\n",
    "    emit_nvtx=torch.cuda.is_available(),\n",
    "    activities=(\n",
    "        [\n",
    "            torch.profiler.ProfilerActivity.CPU,\n",
    "            torch.profiler.ProfilerActivity.CUDA,\n",
    "        ]\n",
    "        if torch.cuda.is_available()\n",
    "        else [\n",
    "            torch.profiler.ProfilerActivity.CPU,\n",
    "        ]\n",
    "    ),\n",
    "    schedule=torch.profiler.schedule(\n",
    "        wait=1, warmup=1, active=5, repeat=3, skip_first=True\n",
    "    ),\n",
    "    profile_memory=True,\n",
    "    with_stack=True,\n",
    "    with_flops=True,\n",
    "    with_modules=True,\n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler(\n",
    "        str(os.path.join(perf_dir, \"trace\"))\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dm = LitAuthorData(\n",
    "    file_path=os.path.join(os.getcwd(), \"dataum\", f\"input.txt.keep\"),\n",
    "    block_size=CONFIG[\"data\"].get(\"seq_len\"),\n",
    "    batch_size=CONFIG[\"data\"].get(\"batch_size\"),\n",
    "    num_workers=CONFIG[\"data\"].get(\"num_workers\"),\n",
    ")\n",
    "\n",
    "dm.prepare_data()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NanoGPT Model\n",
    "model = NanoGPT(\n",
    "    d_model=CONFIG[\"model\"].get(\"d_model\"),\n",
    "    seq_len=CONFIG[\"data\"].get(\"seq_len\"),\n",
    "    vocab_size=dm.train_ds.vocab_size,\n",
    "    n_head=CONFIG[\"model\"].get(\"n_head\"),\n",
    "    n_layer=CONFIG[\"model\"].get(\"n_layer\"),\n",
    "    lr=CONFIG[\"lr\"],\n",
    "    bias=False,\n",
    "    dropout_rate=float(CONFIG[\"model\"].get(\"dropout\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHKPOINT_PATH:str = os.path.join('logs','chkpoints','epoch=04.ckpt')\n",
    "CHKPOINT:dict     = torch.load(CHKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(CHKPOINT['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=CONFIG[\"trainer\"].get(\"epoch\"),\n",
    "    callbacks=call_backs,\n",
    "    logger=logger,\n",
    "    precision='16-mixed',\n",
    "    profiler='pytorch',#perf_profiler,#'advanced',\n",
    "    enable_model_summary=True,\n",
    "    enable_progress_bar=True,\n",
    "    accumulate_grad_batches=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================================\n",
      "Layer (type (var_name))                            Input Shape      Output Shape     Param #          Kernel Shape     Mult-Adds\n",
      "==================================================================================================================================\n",
      "NanoGPT (NanoGPT)                                  [64, 64]         [64, 64, 65]     --               --               --\n",
      "â”œâ”€InputEmbeddings (txt_embedding)                  [64, 64]         [64, 64, 64]     --               --               --\n",
      "â”‚    â””â”€embedding.weight                                                              â””â”€4,160          [65, 64]\n",
      "â”‚    â””â”€Embedding (embedding)                       [64, 64]         [64, 64, 64]     4,160            --               266,240\n",
      "â”‚    â”‚    â””â”€weight                                                                   â””â”€4,160          [64, 65]\n",
      "â”œâ”€PositionalEncoding (pos_embedding)               [64, 64, 64]     [64, 64, 64]     --               --               --\n",
      "â”‚    â””â”€Dropout (dropout)                           [64, 64, 64]     [64, 64, 64]     --               --               --\n",
      "â”œâ”€Sequential (decoder)                             [64, 64, 64]     [64, 64, 64]     --               --               --\n",
      "â”‚    â””â”€0.multi_attn.heads.0.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
      "â”‚    â””â”€0.multi_attn.heads.1.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
      "â”‚    â””â”€0.multi_attn.heads.2.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
      "â”‚    â””â”€0.multi_attn.heads.3.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
      "â”‚    â””â”€0.multi_attn.out.weight                                                       â”œâ”€4,096          [64, 64]\n",
      "â”‚    â””â”€0.layernorm1.alpha                                                            â”œâ”€1              [1]\n",
      "â”‚    â””â”€0.layernorm1.beta                                                             â”œâ”€1              [1]\n",
      "â”‚    â””â”€0.layernorm2.alpha                                                            â”œâ”€1              [1]\n",
      "â”‚    â””â”€0.layernorm2.beta                                                             â”œâ”€1              [1]\n",
      "â”‚    â””â”€0.feedforward.linear.0.weight                                                 â”œâ”€16,384         [256, 64]\n",
      "â”‚    â””â”€0.feedforward.linear.3.weight                                                 â”œâ”€16,384         [64, 256]\n",
      "â”‚    â””â”€1.multi_attn.heads.0.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
      "â”‚    â””â”€1.multi_attn.heads.1.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
      "â”‚    â””â”€1.multi_attn.heads.2.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
      "â”‚    â””â”€1.multi_attn.heads.3.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
      "â”‚    â””â”€1.multi_attn.out.weight                                                       â”œâ”€4,096          [64, 64]\n",
      "â”‚    â””â”€1.layernorm1.alpha                                                            â”œâ”€1              [1]\n",
      "â”‚    â””â”€1.layernorm1.beta                                                             â”œâ”€1              [1]\n",
      "â”‚    â””â”€1.layernorm2.alpha                                                            â”œâ”€1              [1]\n",
      "â”‚    â””â”€1.layernorm2.beta                                                             â”œâ”€1              [1]\n",
      "â”‚    â””â”€1.feedforward.linear.0.weight                                                 â”œâ”€16,384         [256, 64]\n",
      "â”‚    â””â”€1.feedforward.linear.3.weight                                                 â”œâ”€16,384         [64, 256]\n",
      "â”‚    â””â”€2.multi_attn.heads.0.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
      "â”‚    â””â”€2.multi_attn.heads.1.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
      "â”‚    â””â”€2.multi_attn.heads.2.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
      "â”‚    â””â”€2.multi_attn.heads.3.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
      "â”‚    â””â”€2.multi_attn.out.weight                                                       â”œâ”€4,096          [64, 64]\n",
      "â”‚    â””â”€2.layernorm1.alpha                                                            â”œâ”€1              [1]\n",
      "â”‚    â””â”€2.layernorm1.beta                                                             â”œâ”€1              [1]\n",
      "â”‚    â””â”€2.layernorm2.alpha                                                            â”œâ”€1              [1]\n",
      "â”‚    â””â”€2.layernorm2.beta                                                             â”œâ”€1              [1]\n",
      "â”‚    â””â”€2.feedforward.linear.0.weight                                                 â”œâ”€16,384         [256, 64]\n",
      "â”‚    â””â”€2.feedforward.linear.3.weight                                                 â”œâ”€16,384         [64, 256]\n",
      "â”‚    â””â”€3.multi_attn.heads.0.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
      "â”‚    â””â”€3.multi_attn.heads.1.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
      "â”‚    â””â”€3.multi_attn.heads.2.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
      "â”‚    â””â”€3.multi_attn.heads.3.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
      "â”‚    â””â”€3.multi_attn.out.weight                                                       â”œâ”€4,096          [64, 64]\n",
      "â”‚    â””â”€3.layernorm1.alpha                                                            â”œâ”€1              [1]\n",
      "â”‚    â””â”€3.layernorm1.beta                                                             â”œâ”€1              [1]\n",
      "â”‚    â””â”€3.layernorm2.alpha                                                            â”œâ”€1              [1]\n",
      "â”‚    â””â”€3.layernorm2.beta                                                             â”œâ”€1              [1]\n",
      "â”‚    â””â”€3.feedforward.linear.0.weight                                                 â”œâ”€16,384         [256, 64]\n",
      "â”‚    â””â”€3.feedforward.linear.3.weight                                                 â””â”€16,384         [64, 256]\n",
      "â”‚    â””â”€DecoderBlock (0)                            [64, 64, 64]     [64, 64, 64]     --               --               --\n",
      "â”‚    â”‚    â””â”€multi_attn.heads.0.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â””â”€multi_attn.heads.1.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â””â”€multi_attn.heads.2.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â””â”€multi_attn.heads.3.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â””â”€multi_attn.out.weight                                                    â”œâ”€4,096          [64, 64]\n",
      "â”‚    â”‚    â””â”€layernorm1.alpha                                                         â”œâ”€1              [1]\n",
      "â”‚    â”‚    â””â”€layernorm1.beta                                                          â”œâ”€1              [1]\n",
      "â”‚    â”‚    â””â”€layernorm2.alpha                                                         â”œâ”€1              [1]\n",
      "â”‚    â”‚    â””â”€layernorm2.beta                                                          â”œâ”€1              [1]\n",
      "â”‚    â”‚    â””â”€feedforward.linear.0.weight                                              â”œâ”€16,384         [256, 64]\n",
      "â”‚    â”‚    â””â”€feedforward.linear.3.weight                                              â””â”€16,384         [64, 256]\n",
      "â”‚    â”‚    â””â”€LayerNormalization (layernorm1)        [64, 64, 64]     [64, 64, 64]     2                --               --\n",
      "â”‚    â”‚    â”‚    â””â”€alpha                                                               â”œâ”€1              [1]\n",
      "â”‚    â”‚    â”‚    â””â”€beta                                                                â””â”€1              [1]\n",
      "â”‚    â”‚    â””â”€MultiHeadAttention (multi_attn)        [64, 64, 64]     [64, 64, 64]     --               --               --\n",
      "â”‚    â”‚    â”‚    â””â”€heads.0.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€heads.1.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€heads.2.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€heads.3.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€out.weight                                                          â””â”€4,096          [64, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€ModuleList (heads)                --               --               --               --               --\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€0.c_attn.weight                                                â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€1.c_attn.weight                                                â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€2.c_attn.weight                                                â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€3.c_attn.weight                                                â””â”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (0)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
      "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (1)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
      "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (2)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
      "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (3)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
      "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€Linear (out)                      [64, 64, 64]     [64, 64, 64]     4,096            --               262,144\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€weight                                                         â””â”€4,096          [64, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€Dropout (dropout)                 [64, 64, 64]     [64, 64, 64]     --               --               --\n",
      "â”‚    â”‚    â””â”€LayerNormalization (layernorm2)        [64, 64, 64]     [64, 64, 64]     2                --               --\n",
      "â”‚    â”‚    â”‚    â””â”€alpha                                                               â”œâ”€1              [1]\n",
      "â”‚    â”‚    â”‚    â””â”€beta                                                                â””â”€1              [1]\n",
      "â”‚    â”‚    â””â”€FeedForwardBlock (feedforward)         [64, 64, 64]     [64, 64, 64]     --               --               --\n",
      "â”‚    â”‚    â”‚    â””â”€linear.0.weight                                                     â”œâ”€16,384         [256, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€linear.3.weight                                                     â””â”€16,384         [64, 256]\n",
      "â”‚    â”‚    â”‚    â””â”€Sequential (linear)               [64, 64, 64]     [64, 64, 64]     --               --               --\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€0.weight                                                       â”œâ”€16,384         [256, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€3.weight                                                       â””â”€16,384         [64, 256]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Linear (0)                   [64, 64, 64]     [64, 64, 256]    16,384           --               1,048,576\n",
      "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€weight                                                    â””â”€16,384         [64, 256]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€ReLU (1)                     [64, 64, 256]    [64, 64, 256]    --               --               --\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Dropout (2)                  [64, 64, 256]    [64, 64, 256]    --               --               --\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Linear (3)                   [64, 64, 256]    [64, 64, 64]     16,384           --               1,048,576\n",
      "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€weight                                                    â””â”€16,384         [256, 64]\n",
      "â”‚    â””â”€DecoderBlock (1)                            [64, 64, 64]     [64, 64, 64]     --               --               --\n",
      "â”‚    â”‚    â””â”€multi_attn.heads.0.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â””â”€multi_attn.heads.1.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â””â”€multi_attn.heads.2.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â””â”€multi_attn.heads.3.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â””â”€multi_attn.out.weight                                                    â”œâ”€4,096          [64, 64]\n",
      "â”‚    â”‚    â””â”€layernorm1.alpha                                                         â”œâ”€1              [1]\n",
      "â”‚    â”‚    â””â”€layernorm1.beta                                                          â”œâ”€1              [1]\n",
      "â”‚    â”‚    â””â”€layernorm2.alpha                                                         â”œâ”€1              [1]\n",
      "â”‚    â”‚    â””â”€layernorm2.beta                                                          â”œâ”€1              [1]\n",
      "â”‚    â”‚    â””â”€feedforward.linear.0.weight                                              â”œâ”€16,384         [256, 64]\n",
      "â”‚    â”‚    â””â”€feedforward.linear.3.weight                                              â””â”€16,384         [64, 256]\n",
      "â”‚    â”‚    â””â”€LayerNormalization (layernorm1)        [64, 64, 64]     [64, 64, 64]     2                --               --\n",
      "â”‚    â”‚    â”‚    â””â”€alpha                                                               â”œâ”€1              [1]\n",
      "â”‚    â”‚    â”‚    â””â”€beta                                                                â””â”€1              [1]\n",
      "â”‚    â”‚    â””â”€MultiHeadAttention (multi_attn)        [64, 64, 64]     [64, 64, 64]     --               --               --\n",
      "â”‚    â”‚    â”‚    â””â”€heads.0.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€heads.1.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€heads.2.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€heads.3.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€out.weight                                                          â””â”€4,096          [64, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€ModuleList (heads)                --               --               --               --               --\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€0.c_attn.weight                                                â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€1.c_attn.weight                                                â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€2.c_attn.weight                                                â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€3.c_attn.weight                                                â””â”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (0)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
      "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (1)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
      "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (2)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
      "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (3)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
      "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€Linear (out)                      [64, 64, 64]     [64, 64, 64]     4,096            --               262,144\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€weight                                                         â””â”€4,096          [64, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€Dropout (dropout)                 [64, 64, 64]     [64, 64, 64]     --               --               --\n",
      "â”‚    â”‚    â””â”€LayerNormalization (layernorm2)        [64, 64, 64]     [64, 64, 64]     2                --               --\n",
      "â”‚    â”‚    â”‚    â””â”€alpha                                                               â”œâ”€1              [1]\n",
      "â”‚    â”‚    â”‚    â””â”€beta                                                                â””â”€1              [1]\n",
      "â”‚    â”‚    â””â”€FeedForwardBlock (feedforward)         [64, 64, 64]     [64, 64, 64]     --               --               --\n",
      "â”‚    â”‚    â”‚    â””â”€linear.0.weight                                                     â”œâ”€16,384         [256, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€linear.3.weight                                                     â””â”€16,384         [64, 256]\n",
      "â”‚    â”‚    â”‚    â””â”€Sequential (linear)               [64, 64, 64]     [64, 64, 64]     --               --               --\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€0.weight                                                       â”œâ”€16,384         [256, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€3.weight                                                       â””â”€16,384         [64, 256]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Linear (0)                   [64, 64, 64]     [64, 64, 256]    16,384           --               1,048,576\n",
      "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€weight                                                    â””â”€16,384         [64, 256]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€ReLU (1)                     [64, 64, 256]    [64, 64, 256]    --               --               --\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Dropout (2)                  [64, 64, 256]    [64, 64, 256]    --               --               --\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Linear (3)                   [64, 64, 256]    [64, 64, 64]     16,384           --               1,048,576\n",
      "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€weight                                                    â””â”€16,384         [256, 64]\n",
      "â”‚    â””â”€DecoderBlock (2)                            [64, 64, 64]     [64, 64, 64]     --               --               --\n",
      "â”‚    â”‚    â””â”€multi_attn.heads.0.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â””â”€multi_attn.heads.1.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â””â”€multi_attn.heads.2.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â””â”€multi_attn.heads.3.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â””â”€multi_attn.out.weight                                                    â”œâ”€4,096          [64, 64]\n",
      "â”‚    â”‚    â””â”€layernorm1.alpha                                                         â”œâ”€1              [1]\n",
      "â”‚    â”‚    â””â”€layernorm1.beta                                                          â”œâ”€1              [1]\n",
      "â”‚    â”‚    â””â”€layernorm2.alpha                                                         â”œâ”€1              [1]\n",
      "â”‚    â”‚    â””â”€layernorm2.beta                                                          â”œâ”€1              [1]\n",
      "â”‚    â”‚    â””â”€feedforward.linear.0.weight                                              â”œâ”€16,384         [256, 64]\n",
      "â”‚    â”‚    â””â”€feedforward.linear.3.weight                                              â””â”€16,384         [64, 256]\n",
      "â”‚    â”‚    â””â”€LayerNormalization (layernorm1)        [64, 64, 64]     [64, 64, 64]     2                --               --\n",
      "â”‚    â”‚    â”‚    â””â”€alpha                                                               â”œâ”€1              [1]\n",
      "â”‚    â”‚    â”‚    â””â”€beta                                                                â””â”€1              [1]\n",
      "â”‚    â”‚    â””â”€MultiHeadAttention (multi_attn)        [64, 64, 64]     [64, 64, 64]     --               --               --\n",
      "â”‚    â”‚    â”‚    â””â”€heads.0.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€heads.1.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€heads.2.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€heads.3.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€out.weight                                                          â””â”€4,096          [64, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€ModuleList (heads)                --               --               --               --               --\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€0.c_attn.weight                                                â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€1.c_attn.weight                                                â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€2.c_attn.weight                                                â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€3.c_attn.weight                                                â””â”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (0)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
      "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (1)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
      "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (2)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
      "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (3)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
      "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€Linear (out)                      [64, 64, 64]     [64, 64, 64]     4,096            --               262,144\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€weight                                                         â””â”€4,096          [64, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€Dropout (dropout)                 [64, 64, 64]     [64, 64, 64]     --               --               --\n",
      "â”‚    â”‚    â””â”€LayerNormalization (layernorm2)        [64, 64, 64]     [64, 64, 64]     2                --               --\n",
      "â”‚    â”‚    â”‚    â””â”€alpha                                                               â”œâ”€1              [1]\n",
      "â”‚    â”‚    â”‚    â””â”€beta                                                                â””â”€1              [1]\n",
      "â”‚    â”‚    â””â”€FeedForwardBlock (feedforward)         [64, 64, 64]     [64, 64, 64]     --               --               --\n",
      "â”‚    â”‚    â”‚    â””â”€linear.0.weight                                                     â”œâ”€16,384         [256, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€linear.3.weight                                                     â””â”€16,384         [64, 256]\n",
      "â”‚    â”‚    â”‚    â””â”€Sequential (linear)               [64, 64, 64]     [64, 64, 64]     --               --               --\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€0.weight                                                       â”œâ”€16,384         [256, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€3.weight                                                       â””â”€16,384         [64, 256]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Linear (0)                   [64, 64, 64]     [64, 64, 256]    16,384           --               1,048,576\n",
      "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€weight                                                    â””â”€16,384         [64, 256]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€ReLU (1)                     [64, 64, 256]    [64, 64, 256]    --               --               --\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Dropout (2)                  [64, 64, 256]    [64, 64, 256]    --               --               --\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Linear (3)                   [64, 64, 256]    [64, 64, 64]     16,384           --               1,048,576\n",
      "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€weight                                                    â””â”€16,384         [256, 64]\n",
      "â”‚    â””â”€DecoderBlock (3)                            [64, 64, 64]     [64, 64, 64]     --               --               --\n",
      "â”‚    â”‚    â””â”€multi_attn.heads.0.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â””â”€multi_attn.heads.1.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â””â”€multi_attn.heads.2.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â””â”€multi_attn.heads.3.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â””â”€multi_attn.out.weight                                                    â”œâ”€4,096          [64, 64]\n",
      "â”‚    â”‚    â””â”€layernorm1.alpha                                                         â”œâ”€1              [1]\n",
      "â”‚    â”‚    â””â”€layernorm1.beta                                                          â”œâ”€1              [1]\n",
      "â”‚    â”‚    â””â”€layernorm2.alpha                                                         â”œâ”€1              [1]\n",
      "â”‚    â”‚    â””â”€layernorm2.beta                                                          â”œâ”€1              [1]\n",
      "â”‚    â”‚    â””â”€feedforward.linear.0.weight                                              â”œâ”€16,384         [256, 64]\n",
      "â”‚    â”‚    â””â”€feedforward.linear.3.weight                                              â””â”€16,384         [64, 256]\n",
      "â”‚    â”‚    â””â”€LayerNormalization (layernorm1)        [64, 64, 64]     [64, 64, 64]     2                --               --\n",
      "â”‚    â”‚    â”‚    â””â”€alpha                                                               â”œâ”€1              [1]\n",
      "â”‚    â”‚    â”‚    â””â”€beta                                                                â””â”€1              [1]\n",
      "â”‚    â”‚    â””â”€MultiHeadAttention (multi_attn)        [64, 64, 64]     [64, 64, 64]     --               --               --\n",
      "â”‚    â”‚    â”‚    â””â”€heads.0.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€heads.1.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€heads.2.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€heads.3.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€out.weight                                                          â””â”€4,096          [64, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€ModuleList (heads)                --               --               --               --               --\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€0.c_attn.weight                                                â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€1.c_attn.weight                                                â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€2.c_attn.weight                                                â”œâ”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€3.c_attn.weight                                                â””â”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (0)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
      "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (1)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
      "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (2)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
      "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (3)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
      "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€Linear (out)                      [64, 64, 64]     [64, 64, 64]     4,096            --               262,144\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€weight                                                         â””â”€4,096          [64, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€Dropout (dropout)                 [64, 64, 64]     [64, 64, 64]     --               --               --\n",
      "â”‚    â”‚    â””â”€LayerNormalization (layernorm2)        [64, 64, 64]     [64, 64, 64]     2                --               --\n",
      "â”‚    â”‚    â”‚    â””â”€alpha                                                               â”œâ”€1              [1]\n",
      "â”‚    â”‚    â”‚    â””â”€beta                                                                â””â”€1              [1]\n",
      "â”‚    â”‚    â””â”€FeedForwardBlock (feedforward)         [64, 64, 64]     [64, 64, 64]     --               --               --\n",
      "â”‚    â”‚    â”‚    â””â”€linear.0.weight                                                     â”œâ”€16,384         [256, 64]\n",
      "â”‚    â”‚    â”‚    â””â”€linear.3.weight                                                     â””â”€16,384         [64, 256]\n",
      "â”‚    â”‚    â”‚    â””â”€Sequential (linear)               [64, 64, 64]     [64, 64, 64]     --               --               --\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€0.weight                                                       â”œâ”€16,384         [256, 64]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€3.weight                                                       â””â”€16,384         [64, 256]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Linear (0)                   [64, 64, 64]     [64, 64, 256]    16,384           --               1,048,576\n",
      "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€weight                                                    â””â”€16,384         [64, 256]\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€ReLU (1)                     [64, 64, 256]    [64, 64, 256]    --               --               --\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Dropout (2)                  [64, 64, 256]    [64, 64, 256]    --               --               --\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€Linear (3)                   [64, 64, 256]    [64, 64, 64]     16,384           --               1,048,576\n",
      "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€weight                                                    â””â”€16,384         [256, 64]\n",
      "â”œâ”€LayerNormalization (layernorm)                   [64, 64, 64]     [64, 64, 64]     2                --               --\n",
      "â”‚    â””â”€alpha                                                                         â”œâ”€1              [1]\n",
      "â”‚    â””â”€beta                                                                          â””â”€1              [1]\n",
      "â”œâ”€ProjectionLayer (projection)                     [64, 64, 64]     [64, 64, 65]     --               --               --\n",
      "â”‚    â””â”€projection.weight                                                             â”œâ”€4,160          [65, 64]\n",
      "â”‚    â””â”€projection.bias                                                               â””â”€65             [65]\n",
      "â”‚    â””â”€Linear (projection)                         [64, 64, 64]     [64, 64, 65]     4,225            --               270,400\n",
      "â”‚    â”‚    â””â”€weight                                                                   â”œâ”€4,160          [64, 65]\n",
      "â”‚    â”‚    â””â”€bias                                                                     â””â”€65             [65]\n",
      "==================================================================================================================================\n",
      "Total params: 205,011\n",
      "Trainable params: 205,011\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 13.12\n",
      "==================================================================================================================================\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 98.60\n",
      "Params size (MB): 0.82\n",
      "Estimated Total Size (MB): 99.45\n",
      "==================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type (var_name))                            Input Shape      Output Shape     Param #          Kernel Shape     Mult-Adds\n",
       "==================================================================================================================================\n",
       "NanoGPT (NanoGPT)                                  [64, 64]         [64, 64, 65]     --               --               --\n",
       "â”œâ”€InputEmbeddings (txt_embedding)                  [64, 64]         [64, 64, 64]     --               --               --\n",
       "â”‚    â””â”€embedding.weight                                                              â””â”€4,160          [65, 64]\n",
       "â”‚    â””â”€Embedding (embedding)                       [64, 64]         [64, 64, 64]     4,160            --               266,240\n",
       "â”‚    â”‚    â””â”€weight                                                                   â””â”€4,160          [64, 65]\n",
       "â”œâ”€PositionalEncoding (pos_embedding)               [64, 64, 64]     [64, 64, 64]     --               --               --\n",
       "â”‚    â””â”€Dropout (dropout)                           [64, 64, 64]     [64, 64, 64]     --               --               --\n",
       "â”œâ”€Sequential (decoder)                             [64, 64, 64]     [64, 64, 64]     --               --               --\n",
       "â”‚    â””â”€0.multi_attn.heads.0.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
       "â”‚    â””â”€0.multi_attn.heads.1.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
       "â”‚    â””â”€0.multi_attn.heads.2.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
       "â”‚    â””â”€0.multi_attn.heads.3.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
       "â”‚    â””â”€0.multi_attn.out.weight                                                       â”œâ”€4,096          [64, 64]\n",
       "â”‚    â””â”€0.layernorm1.alpha                                                            â”œâ”€1              [1]\n",
       "â”‚    â””â”€0.layernorm1.beta                                                             â”œâ”€1              [1]\n",
       "â”‚    â””â”€0.layernorm2.alpha                                                            â”œâ”€1              [1]\n",
       "â”‚    â””â”€0.layernorm2.beta                                                             â”œâ”€1              [1]\n",
       "â”‚    â””â”€0.feedforward.linear.0.weight                                                 â”œâ”€16,384         [256, 64]\n",
       "â”‚    â””â”€0.feedforward.linear.3.weight                                                 â”œâ”€16,384         [64, 256]\n",
       "â”‚    â””â”€1.multi_attn.heads.0.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
       "â”‚    â””â”€1.multi_attn.heads.1.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
       "â”‚    â””â”€1.multi_attn.heads.2.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
       "â”‚    â””â”€1.multi_attn.heads.3.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
       "â”‚    â””â”€1.multi_attn.out.weight                                                       â”œâ”€4,096          [64, 64]\n",
       "â”‚    â””â”€1.layernorm1.alpha                                                            â”œâ”€1              [1]\n",
       "â”‚    â””â”€1.layernorm1.beta                                                             â”œâ”€1              [1]\n",
       "â”‚    â””â”€1.layernorm2.alpha                                                            â”œâ”€1              [1]\n",
       "â”‚    â””â”€1.layernorm2.beta                                                             â”œâ”€1              [1]\n",
       "â”‚    â””â”€1.feedforward.linear.0.weight                                                 â”œâ”€16,384         [256, 64]\n",
       "â”‚    â””â”€1.feedforward.linear.3.weight                                                 â”œâ”€16,384         [64, 256]\n",
       "â”‚    â””â”€2.multi_attn.heads.0.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
       "â”‚    â””â”€2.multi_attn.heads.1.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
       "â”‚    â””â”€2.multi_attn.heads.2.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
       "â”‚    â””â”€2.multi_attn.heads.3.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
       "â”‚    â””â”€2.multi_attn.out.weight                                                       â”œâ”€4,096          [64, 64]\n",
       "â”‚    â””â”€2.layernorm1.alpha                                                            â”œâ”€1              [1]\n",
       "â”‚    â””â”€2.layernorm1.beta                                                             â”œâ”€1              [1]\n",
       "â”‚    â””â”€2.layernorm2.alpha                                                            â”œâ”€1              [1]\n",
       "â”‚    â””â”€2.layernorm2.beta                                                             â”œâ”€1              [1]\n",
       "â”‚    â””â”€2.feedforward.linear.0.weight                                                 â”œâ”€16,384         [256, 64]\n",
       "â”‚    â””â”€2.feedforward.linear.3.weight                                                 â”œâ”€16,384         [64, 256]\n",
       "â”‚    â””â”€3.multi_attn.heads.0.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
       "â”‚    â””â”€3.multi_attn.heads.1.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
       "â”‚    â””â”€3.multi_attn.heads.2.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
       "â”‚    â””â”€3.multi_attn.heads.3.c_attn.weight                                            â”œâ”€3,072          [48, 64]\n",
       "â”‚    â””â”€3.multi_attn.out.weight                                                       â”œâ”€4,096          [64, 64]\n",
       "â”‚    â””â”€3.layernorm1.alpha                                                            â”œâ”€1              [1]\n",
       "â”‚    â””â”€3.layernorm1.beta                                                             â”œâ”€1              [1]\n",
       "â”‚    â””â”€3.layernorm2.alpha                                                            â”œâ”€1              [1]\n",
       "â”‚    â””â”€3.layernorm2.beta                                                             â”œâ”€1              [1]\n",
       "â”‚    â””â”€3.feedforward.linear.0.weight                                                 â”œâ”€16,384         [256, 64]\n",
       "â”‚    â””â”€3.feedforward.linear.3.weight                                                 â””â”€16,384         [64, 256]\n",
       "â”‚    â””â”€DecoderBlock (0)                            [64, 64, 64]     [64, 64, 64]     --               --               --\n",
       "â”‚    â”‚    â””â”€multi_attn.heads.0.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â””â”€multi_attn.heads.1.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â””â”€multi_attn.heads.2.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â””â”€multi_attn.heads.3.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â””â”€multi_attn.out.weight                                                    â”œâ”€4,096          [64, 64]\n",
       "â”‚    â”‚    â””â”€layernorm1.alpha                                                         â”œâ”€1              [1]\n",
       "â”‚    â”‚    â””â”€layernorm1.beta                                                          â”œâ”€1              [1]\n",
       "â”‚    â”‚    â””â”€layernorm2.alpha                                                         â”œâ”€1              [1]\n",
       "â”‚    â”‚    â””â”€layernorm2.beta                                                          â”œâ”€1              [1]\n",
       "â”‚    â”‚    â””â”€feedforward.linear.0.weight                                              â”œâ”€16,384         [256, 64]\n",
       "â”‚    â”‚    â””â”€feedforward.linear.3.weight                                              â””â”€16,384         [64, 256]\n",
       "â”‚    â”‚    â””â”€LayerNormalization (layernorm1)        [64, 64, 64]     [64, 64, 64]     2                --               --\n",
       "â”‚    â”‚    â”‚    â””â”€alpha                                                               â”œâ”€1              [1]\n",
       "â”‚    â”‚    â”‚    â””â”€beta                                                                â””â”€1              [1]\n",
       "â”‚    â”‚    â””â”€MultiHeadAttention (multi_attn)        [64, 64, 64]     [64, 64, 64]     --               --               --\n",
       "â”‚    â”‚    â”‚    â””â”€heads.0.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€heads.1.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€heads.2.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€heads.3.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€out.weight                                                          â””â”€4,096          [64, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€ModuleList (heads)                --               --               --               --               --\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€0.c_attn.weight                                                â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€1.c_attn.weight                                                â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€2.c_attn.weight                                                â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€3.c_attn.weight                                                â””â”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (0)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
       "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (1)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
       "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (2)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
       "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (3)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
       "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€Linear (out)                      [64, 64, 64]     [64, 64, 64]     4,096            --               262,144\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€weight                                                         â””â”€4,096          [64, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€Dropout (dropout)                 [64, 64, 64]     [64, 64, 64]     --               --               --\n",
       "â”‚    â”‚    â””â”€LayerNormalization (layernorm2)        [64, 64, 64]     [64, 64, 64]     2                --               --\n",
       "â”‚    â”‚    â”‚    â””â”€alpha                                                               â”œâ”€1              [1]\n",
       "â”‚    â”‚    â”‚    â””â”€beta                                                                â””â”€1              [1]\n",
       "â”‚    â”‚    â””â”€FeedForwardBlock (feedforward)         [64, 64, 64]     [64, 64, 64]     --               --               --\n",
       "â”‚    â”‚    â”‚    â””â”€linear.0.weight                                                     â”œâ”€16,384         [256, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€linear.3.weight                                                     â””â”€16,384         [64, 256]\n",
       "â”‚    â”‚    â”‚    â””â”€Sequential (linear)               [64, 64, 64]     [64, 64, 64]     --               --               --\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€0.weight                                                       â”œâ”€16,384         [256, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€3.weight                                                       â””â”€16,384         [64, 256]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Linear (0)                   [64, 64, 64]     [64, 64, 256]    16,384           --               1,048,576\n",
       "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€weight                                                    â””â”€16,384         [64, 256]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€ReLU (1)                     [64, 64, 256]    [64, 64, 256]    --               --               --\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Dropout (2)                  [64, 64, 256]    [64, 64, 256]    --               --               --\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Linear (3)                   [64, 64, 256]    [64, 64, 64]     16,384           --               1,048,576\n",
       "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€weight                                                    â””â”€16,384         [256, 64]\n",
       "â”‚    â””â”€DecoderBlock (1)                            [64, 64, 64]     [64, 64, 64]     --               --               --\n",
       "â”‚    â”‚    â””â”€multi_attn.heads.0.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â””â”€multi_attn.heads.1.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â””â”€multi_attn.heads.2.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â””â”€multi_attn.heads.3.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â””â”€multi_attn.out.weight                                                    â”œâ”€4,096          [64, 64]\n",
       "â”‚    â”‚    â””â”€layernorm1.alpha                                                         â”œâ”€1              [1]\n",
       "â”‚    â”‚    â””â”€layernorm1.beta                                                          â”œâ”€1              [1]\n",
       "â”‚    â”‚    â””â”€layernorm2.alpha                                                         â”œâ”€1              [1]\n",
       "â”‚    â”‚    â””â”€layernorm2.beta                                                          â”œâ”€1              [1]\n",
       "â”‚    â”‚    â””â”€feedforward.linear.0.weight                                              â”œâ”€16,384         [256, 64]\n",
       "â”‚    â”‚    â””â”€feedforward.linear.3.weight                                              â””â”€16,384         [64, 256]\n",
       "â”‚    â”‚    â””â”€LayerNormalization (layernorm1)        [64, 64, 64]     [64, 64, 64]     2                --               --\n",
       "â”‚    â”‚    â”‚    â””â”€alpha                                                               â”œâ”€1              [1]\n",
       "â”‚    â”‚    â”‚    â””â”€beta                                                                â””â”€1              [1]\n",
       "â”‚    â”‚    â””â”€MultiHeadAttention (multi_attn)        [64, 64, 64]     [64, 64, 64]     --               --               --\n",
       "â”‚    â”‚    â”‚    â””â”€heads.0.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€heads.1.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€heads.2.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€heads.3.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€out.weight                                                          â””â”€4,096          [64, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€ModuleList (heads)                --               --               --               --               --\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€0.c_attn.weight                                                â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€1.c_attn.weight                                                â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€2.c_attn.weight                                                â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€3.c_attn.weight                                                â””â”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (0)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
       "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (1)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
       "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (2)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
       "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (3)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
       "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€Linear (out)                      [64, 64, 64]     [64, 64, 64]     4,096            --               262,144\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€weight                                                         â””â”€4,096          [64, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€Dropout (dropout)                 [64, 64, 64]     [64, 64, 64]     --               --               --\n",
       "â”‚    â”‚    â””â”€LayerNormalization (layernorm2)        [64, 64, 64]     [64, 64, 64]     2                --               --\n",
       "â”‚    â”‚    â”‚    â””â”€alpha                                                               â”œâ”€1              [1]\n",
       "â”‚    â”‚    â”‚    â””â”€beta                                                                â””â”€1              [1]\n",
       "â”‚    â”‚    â””â”€FeedForwardBlock (feedforward)         [64, 64, 64]     [64, 64, 64]     --               --               --\n",
       "â”‚    â”‚    â”‚    â””â”€linear.0.weight                                                     â”œâ”€16,384         [256, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€linear.3.weight                                                     â””â”€16,384         [64, 256]\n",
       "â”‚    â”‚    â”‚    â””â”€Sequential (linear)               [64, 64, 64]     [64, 64, 64]     --               --               --\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€0.weight                                                       â”œâ”€16,384         [256, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€3.weight                                                       â””â”€16,384         [64, 256]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Linear (0)                   [64, 64, 64]     [64, 64, 256]    16,384           --               1,048,576\n",
       "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€weight                                                    â””â”€16,384         [64, 256]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€ReLU (1)                     [64, 64, 256]    [64, 64, 256]    --               --               --\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Dropout (2)                  [64, 64, 256]    [64, 64, 256]    --               --               --\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Linear (3)                   [64, 64, 256]    [64, 64, 64]     16,384           --               1,048,576\n",
       "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€weight                                                    â””â”€16,384         [256, 64]\n",
       "â”‚    â””â”€DecoderBlock (2)                            [64, 64, 64]     [64, 64, 64]     --               --               --\n",
       "â”‚    â”‚    â””â”€multi_attn.heads.0.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â””â”€multi_attn.heads.1.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â””â”€multi_attn.heads.2.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â””â”€multi_attn.heads.3.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â””â”€multi_attn.out.weight                                                    â”œâ”€4,096          [64, 64]\n",
       "â”‚    â”‚    â””â”€layernorm1.alpha                                                         â”œâ”€1              [1]\n",
       "â”‚    â”‚    â””â”€layernorm1.beta                                                          â”œâ”€1              [1]\n",
       "â”‚    â”‚    â””â”€layernorm2.alpha                                                         â”œâ”€1              [1]\n",
       "â”‚    â”‚    â””â”€layernorm2.beta                                                          â”œâ”€1              [1]\n",
       "â”‚    â”‚    â””â”€feedforward.linear.0.weight                                              â”œâ”€16,384         [256, 64]\n",
       "â”‚    â”‚    â””â”€feedforward.linear.3.weight                                              â””â”€16,384         [64, 256]\n",
       "â”‚    â”‚    â””â”€LayerNormalization (layernorm1)        [64, 64, 64]     [64, 64, 64]     2                --               --\n",
       "â”‚    â”‚    â”‚    â””â”€alpha                                                               â”œâ”€1              [1]\n",
       "â”‚    â”‚    â”‚    â””â”€beta                                                                â””â”€1              [1]\n",
       "â”‚    â”‚    â””â”€MultiHeadAttention (multi_attn)        [64, 64, 64]     [64, 64, 64]     --               --               --\n",
       "â”‚    â”‚    â”‚    â””â”€heads.0.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€heads.1.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€heads.2.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€heads.3.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€out.weight                                                          â””â”€4,096          [64, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€ModuleList (heads)                --               --               --               --               --\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€0.c_attn.weight                                                â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€1.c_attn.weight                                                â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€2.c_attn.weight                                                â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€3.c_attn.weight                                                â””â”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (0)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
       "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (1)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
       "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (2)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
       "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (3)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
       "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€Linear (out)                      [64, 64, 64]     [64, 64, 64]     4,096            --               262,144\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€weight                                                         â””â”€4,096          [64, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€Dropout (dropout)                 [64, 64, 64]     [64, 64, 64]     --               --               --\n",
       "â”‚    â”‚    â””â”€LayerNormalization (layernorm2)        [64, 64, 64]     [64, 64, 64]     2                --               --\n",
       "â”‚    â”‚    â”‚    â””â”€alpha                                                               â”œâ”€1              [1]\n",
       "â”‚    â”‚    â”‚    â””â”€beta                                                                â””â”€1              [1]\n",
       "â”‚    â”‚    â””â”€FeedForwardBlock (feedforward)         [64, 64, 64]     [64, 64, 64]     --               --               --\n",
       "â”‚    â”‚    â”‚    â””â”€linear.0.weight                                                     â”œâ”€16,384         [256, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€linear.3.weight                                                     â””â”€16,384         [64, 256]\n",
       "â”‚    â”‚    â”‚    â””â”€Sequential (linear)               [64, 64, 64]     [64, 64, 64]     --               --               --\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€0.weight                                                       â”œâ”€16,384         [256, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€3.weight                                                       â””â”€16,384         [64, 256]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Linear (0)                   [64, 64, 64]     [64, 64, 256]    16,384           --               1,048,576\n",
       "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€weight                                                    â””â”€16,384         [64, 256]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€ReLU (1)                     [64, 64, 256]    [64, 64, 256]    --               --               --\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Dropout (2)                  [64, 64, 256]    [64, 64, 256]    --               --               --\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Linear (3)                   [64, 64, 256]    [64, 64, 64]     16,384           --               1,048,576\n",
       "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€weight                                                    â””â”€16,384         [256, 64]\n",
       "â”‚    â””â”€DecoderBlock (3)                            [64, 64, 64]     [64, 64, 64]     --               --               --\n",
       "â”‚    â”‚    â””â”€multi_attn.heads.0.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â””â”€multi_attn.heads.1.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â””â”€multi_attn.heads.2.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â””â”€multi_attn.heads.3.c_attn.weight                                         â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â””â”€multi_attn.out.weight                                                    â”œâ”€4,096          [64, 64]\n",
       "â”‚    â”‚    â””â”€layernorm1.alpha                                                         â”œâ”€1              [1]\n",
       "â”‚    â”‚    â””â”€layernorm1.beta                                                          â”œâ”€1              [1]\n",
       "â”‚    â”‚    â””â”€layernorm2.alpha                                                         â”œâ”€1              [1]\n",
       "â”‚    â”‚    â””â”€layernorm2.beta                                                          â”œâ”€1              [1]\n",
       "â”‚    â”‚    â””â”€feedforward.linear.0.weight                                              â”œâ”€16,384         [256, 64]\n",
       "â”‚    â”‚    â””â”€feedforward.linear.3.weight                                              â””â”€16,384         [64, 256]\n",
       "â”‚    â”‚    â””â”€LayerNormalization (layernorm1)        [64, 64, 64]     [64, 64, 64]     2                --               --\n",
       "â”‚    â”‚    â”‚    â””â”€alpha                                                               â”œâ”€1              [1]\n",
       "â”‚    â”‚    â”‚    â””â”€beta                                                                â””â”€1              [1]\n",
       "â”‚    â”‚    â””â”€MultiHeadAttention (multi_attn)        [64, 64, 64]     [64, 64, 64]     --               --               --\n",
       "â”‚    â”‚    â”‚    â””â”€heads.0.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€heads.1.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€heads.2.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€heads.3.c_attn.weight                                               â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€out.weight                                                          â””â”€4,096          [64, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€ModuleList (heads)                --               --               --               --               --\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€0.c_attn.weight                                                â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€1.c_attn.weight                                                â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€2.c_attn.weight                                                â”œâ”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€3.c_attn.weight                                                â””â”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (0)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
       "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (1)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
       "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (2)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
       "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Attention (3)                [64, 64, 64]     [64, 64, 16]     3,072            --               196,608\n",
       "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€c_attn.weight                                             â””â”€3,072          [48, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€Linear (out)                      [64, 64, 64]     [64, 64, 64]     4,096            --               262,144\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€weight                                                         â””â”€4,096          [64, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€Dropout (dropout)                 [64, 64, 64]     [64, 64, 64]     --               --               --\n",
       "â”‚    â”‚    â””â”€LayerNormalization (layernorm2)        [64, 64, 64]     [64, 64, 64]     2                --               --\n",
       "â”‚    â”‚    â”‚    â””â”€alpha                                                               â”œâ”€1              [1]\n",
       "â”‚    â”‚    â”‚    â””â”€beta                                                                â””â”€1              [1]\n",
       "â”‚    â”‚    â””â”€FeedForwardBlock (feedforward)         [64, 64, 64]     [64, 64, 64]     --               --               --\n",
       "â”‚    â”‚    â”‚    â””â”€linear.0.weight                                                     â”œâ”€16,384         [256, 64]\n",
       "â”‚    â”‚    â”‚    â””â”€linear.3.weight                                                     â””â”€16,384         [64, 256]\n",
       "â”‚    â”‚    â”‚    â””â”€Sequential (linear)               [64, 64, 64]     [64, 64, 64]     --               --               --\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€0.weight                                                       â”œâ”€16,384         [256, 64]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€3.weight                                                       â””â”€16,384         [64, 256]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Linear (0)                   [64, 64, 64]     [64, 64, 256]    16,384           --               1,048,576\n",
       "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€weight                                                    â””â”€16,384         [64, 256]\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€ReLU (1)                     [64, 64, 256]    [64, 64, 256]    --               --               --\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Dropout (2)                  [64, 64, 256]    [64, 64, 256]    --               --               --\n",
       "â”‚    â”‚    â”‚    â”‚    â””â”€Linear (3)                   [64, 64, 256]    [64, 64, 64]     16,384           --               1,048,576\n",
       "â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€weight                                                    â””â”€16,384         [256, 64]\n",
       "â”œâ”€LayerNormalization (layernorm)                   [64, 64, 64]     [64, 64, 64]     2                --               --\n",
       "â”‚    â””â”€alpha                                                                         â”œâ”€1              [1]\n",
       "â”‚    â””â”€beta                                                                          â””â”€1              [1]\n",
       "â”œâ”€ProjectionLayer (projection)                     [64, 64, 64]     [64, 64, 65]     --               --               --\n",
       "â”‚    â””â”€projection.weight                                                             â”œâ”€4,160          [65, 64]\n",
       "â”‚    â””â”€projection.bias                                                               â””â”€65             [65]\n",
       "â”‚    â””â”€Linear (projection)                         [64, 64, 64]     [64, 64, 65]     4,225            --               270,400\n",
       "â”‚    â”‚    â””â”€weight                                                                   â”œâ”€4,160          [64, 65]\n",
       "â”‚    â”‚    â””â”€bias                                                                     â””â”€65             [65]\n",
       "==================================================================================================================================\n",
       "Total params: 205,011\n",
       "Trainable params: 205,011\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 13.12\n",
       "==================================================================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 98.60\n",
       "Params size (MB): 0.82\n",
       "Estimated Total Size (MB): 99.45\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Graph \n",
    "batch = next(iter(dm.train_dataloader()))\n",
    "# ip,op = batch\n",
    "\n",
    "\n",
    "## CPU Stats\n",
    "# with torch.autograd.profiler.profile() as prof:\n",
    "#     output = model.to(device)(batch[0].to(device))\n",
    "\n",
    "# os.makedirs(name=os.path.join(os.path.dirname(__file__),'logs','profiler'),exist_ok=True)\n",
    "# with open(os.path.join(os.path.dirname(__file__),'logs','profiler',\"cpu_throttle.txt\"), \"w\") as text_file:\n",
    "#     text_file.write(f\"{prof.key_averages().table(sort_by='self_cpu_time_total',top_level_events_only=False)}\")\n",
    "\n",
    "\n",
    "\n",
    "## Model Summary\n",
    "summary(\n",
    "    model=model,\n",
    "    input_data=batch[0],\n",
    "    depth=5,\n",
    "    verbose=2,\n",
    "    col_width=16,\n",
    "    col_names=[\n",
    "        \"input_size\",\n",
    "        \"output_size\",\n",
    "        \"num_params\",\n",
    "        \"kernel_size\",\n",
    "        \"mult_adds\",\n",
    "    ],\n",
    "    row_settings=[\"var_names\"],\n",
    ")\n",
    "\n",
    "## Graph\n",
    "# logger.log_graph(model.to(device),(ip.to(device),op.to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\muthu\\miniconda3\\envs\\venv\\Lib\\site-packages\\lightning\\pytorch\\loggers\\tensorboard.py:194: Could not log computational graph to TensorBoard: The `model.example_input_array` attribute is not set or `input_array` was not given.\n",
      "c:\\Users\\muthu\\miniconda3\\envs\\venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1742/1742 [00:34<00:00, 49.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">      Validate metric      </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">      val/loss_epoch       </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.07081184536218643    </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m     val/loss_epoch      \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.07081184536218643   \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALIDATE Profiler Report\n",
      "Profile stats for: records\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          ProfilerStep*         3.36%       6.177ms       100.00%     183.614ms      61.205ms       5.551ms         3.02%     183.739ms      61.246ms             3  \n",
      "[pl][profile][Strategy]SingleDeviceStrategy.validati...         1.42%       2.612ms        82.16%     150.857ms      50.286ms     781.000us         0.43%     150.086ms      50.029ms             3  \n",
      "[pl][module]torch.nn.modules.container.Sequential: d...         0.29%     529.000us        76.93%     141.259ms      47.086ms     163.000us         0.09%     140.633ms      46.878ms             3  \n",
      "                                           aten::matmul        10.00%      18.368ms        27.40%      50.313ms     220.671us      10.932ms         5.95%      52.688ms     231.088us           228  \n",
      "                                           aten::linear         3.96%       7.270ms        24.97%      45.843ms     263.466us       3.904ms         2.12%      55.109ms     316.718us           174  \n",
      "      [pl][module]model.decoder.DecoderBlock: decoder.0         0.50%     926.000us        23.08%      42.373ms      14.124ms     305.000us         0.17%      41.599ms      13.866ms             3  \n",
      "      [pl][module]model.decoder.DecoderBlock: decoder.2         0.77%       1.423ms        19.29%      35.415ms      11.805ms       3.456ms         1.88%      33.800ms      11.267ms             3  \n",
      "[pl][module]model.decoder.MultiHeadAttention: decode...         0.74%       1.356ms        18.94%      34.769ms      11.590ms     757.000us         0.41%      33.960ms      11.320ms             3  \n",
      "      [pl][module]model.decoder.DecoderBlock: decoder.1         0.54%     989.000us        18.41%      33.811ms      11.270ms     259.000us         0.14%      35.636ms      11.879ms             3  \n",
      "      [pl][module]model.decoder.DecoderBlock: decoder.3         0.40%     743.000us        15.87%      29.131ms       9.710ms     196.000us         0.11%      29.435ms       9.812ms             3  \n",
      "[pl][module]model.decoder.MultiHeadAttention: decode...         0.53%     971.000us        15.63%      28.691ms       9.564ms     471.000us         0.26%      24.405ms       8.135ms             3  \n",
      "[pl][module]model.decoder.MultiHeadAttention: decode...         0.52%     957.000us        13.49%      24.772ms       8.257ms     572.000us         0.31%      26.309ms       8.770ms             3  \n",
      "                [pl][profile][_EvaluationLoop].val_next         0.10%     185.000us        13.20%      24.233ms       8.078ms      23.000us         0.01%      25.739ms       8.580ms             3  \n",
      "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...         7.26%      13.324ms        13.10%      24.048ms       8.016ms       7.914ms         4.31%      25.716ms       8.572ms             3  \n",
      "[pl][module]model.decoder.MultiHeadAttention: decode...         0.50%     915.000us        12.55%      23.037ms       7.679ms     502.000us         0.27%      22.644ms       7.548ms             3  \n",
      "                                               aten::to         2.27%       4.170ms         8.16%      14.984ms      71.352us       3.753ms         2.04%      19.924ms      94.876us           210  \n",
      "                                            aten::slice         7.05%      12.936ms         7.19%      13.196ms      24.712us      12.442ms         6.77%      20.649ms      38.669us           534  \n",
      "                                      aten::masked_fill         1.66%       3.057ms         6.29%      11.544ms     240.500us       2.022ms         1.10%      15.751ms     328.146us            48  \n",
      "                                         aten::_to_copy         3.36%       6.178ms         5.89%      10.814ms      53.801us       3.869ms         2.11%      16.171ms      80.453us           201  \n",
      "[pl][module]model.decoder.Attention: decoder.2.multi...         0.85%       1.568ms         5.31%       9.744ms       3.248ms     466.000us         0.25%       5.312ms       1.771ms             3  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 183.614ms\n",
      "Self CUDA time total: 183.739ms\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val/loss_epoch': 0.07081184536218643}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model,datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generation\n",
    "def generate_text(prompt: str, max_new_token: int, dm=dm, imodel=model):\n",
    "    encoded = dm.train_ds.encode(prompt)\n",
    "    imodel.to(device)\n",
    "    with torch.no_grad():\n",
    "        encoded_text = torch.tensor(encoded,device=device).unsqueeze(0)\n",
    "        new_word_predict = []\n",
    "        for _ in range(max_new_token):\n",
    "            encoded_text = encoded_text[:, -32:]\n",
    "            logits, _ = imodel(encoded_text)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            next_word = torch.multinomial(probs, num_samples=1)\n",
    "            new_word_predict.append(next_word.item())\n",
    "            encoded_text = torch.cat((encoded_text[:, imodel.seq_len:], next_word), dim=1)             # Beam Search in Decodiing Strategy\n",
    "    res = encoded + new_word_predict\n",
    "    return dm.train_ds.decode(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But with prison,, sgupittnnN\n",
      ":h tRjge'LGnA;EeEaHAEEEEEL\n",
      "e  \n",
      " \n",
      "EdlaalllW'''d  HLaa-!Epuuuy3RnnaaAaccrru'aaEacAIA;HFhh'\n",
      "\n",
      " M'kl DDww-c  HpHuaEheOww pTSoHm,.yyy:ea\n",
      "\n",
      "'Ih?FF bSsoaUPujlkZooo-u j   e,,V\n",
      "iZa-AaAEBlffIaaiiYO\n",
      "vfk\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"But with prison,, \",200,dm=dm,imodel=model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ojenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
