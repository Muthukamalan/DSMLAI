{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59733/2331140124.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  torch.cuda.amp.autocast(enabled=True, dtype=torch.float16)\n",
      "Seed set to 270498\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc \n",
    "gc.collect()\n",
    "\n",
    "import torch\n",
    "import lightning as pl\n",
    "from torchinfo import summary\n",
    "\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "# from functorch.compile import compiled_function, draw_graph\n",
    "from lightning.pytorch.profilers import PyTorchProfiler\n",
    "from lightning.pytorch.callbacks import (\n",
    "    # DeviceStatsMonitor,\n",
    "    EarlyStopping,\n",
    "    # LearningRateMonitor,\n",
    "    # ModelCheckpoint,\n",
    "    # ModelPruning,\n",
    ")\n",
    "from lightning.pytorch.callbacks.progress import TQDMProgressBar\n",
    "\n",
    "\n",
    "from config import CONFIG,DEVICE\n",
    "from model import NanoGPT\n",
    "from dataum import LitAuthorData,GPT2LitAuthorData\n",
    "\n",
    "# Auxilary utils\n",
    "torch.backends.cuda.matmul.allow_tf32=True\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "torch.cuda.amp.autocast(enabled=True, dtype=torch.float16)\n",
    "torch.set_default_device(device=DEVICE)\n",
    "torch.cuda.empty_cache()\n",
    "pl.seed_everything(270498);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Loggers\n",
    "# logger: pl_loggers.TensorBoardLogger = pl_loggers.TensorBoardLogger(\n",
    "#     save_dir=\"logs/\", name=\"nanogpt2\", log_graph=True,version= input(\"Enter name of the experiment: \")\n",
    "# )\n",
    "\n",
    "# ## CallBacks\n",
    "# call_backs = [\n",
    "#     TQDMProgressBar(refresh_rate=10),\n",
    "#     ModelCheckpoint(\n",
    "#         monitor=\"val/loss\",                        # val/loss >> train/loss (to get low loss)\n",
    "#         dirpath=os.path.join(\"logs\", \"chkpoints\"),\n",
    "#         filename=\"{epoch:02d}\",\n",
    "#         save_top_k=1\n",
    "#     ),\n",
    "#     DeviceStatsMonitor(cpu_stats=True),\n",
    "#     # EarlyStopping(monitor=\"val/loss\",mode='min'),\n",
    "#     LearningRateMonitor(logging_interval=\"step\"),\n",
    "# ]\n",
    "\n",
    "# ## Profilers\n",
    "# perf_dir = os.path.join(os.getcwd(), \"logs\", \"profiler\")\n",
    "# perf_profiler = PyTorchProfiler(\n",
    "#     dirpath=perf_dir,\n",
    "#     filename=\"perf_logs_pytorch\",\n",
    "#     group_by_input_shapes=True,\n",
    "#     emit_nvtx=torch.cuda.is_available(),\n",
    "#     activities=(\n",
    "#         [\n",
    "#             torch.profiler.ProfilerActivity.CPU,\n",
    "#             torch.profiler.ProfilerActivity.CUDA,\n",
    "#         ]\n",
    "#         if torch.cuda.is_available()\n",
    "#         else [\n",
    "#             torch.profiler.ProfilerActivity.CPU,\n",
    "#         ]\n",
    "#     ),\n",
    "#     schedule=torch.profiler.schedule(\n",
    "#         wait=1, warmup=1, active=5, repeat=3, skip_first=True\n",
    "#     ),\n",
    "#     profile_memory=True,\n",
    "#     with_stack=True,\n",
    "#     with_flops=True,\n",
    "#     with_modules=True,\n",
    "#     on_trace_ready=torch.profiler.tensorboard_trace_handler(\n",
    "#         str(os.path.join(perf_dir, \"trace\"))\n",
    "#     ),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dm = GPT2LitAuthorData(\n",
    "    file_path=os.path.join(os.getcwd(), \"dataum\", f\"input.txt.keep\"),\n",
    "    block_size=CONFIG['data'].get('seq_len'),\n",
    "    batch_size=CONFIG['data'].get('batch_size'),\n",
    "    num_workers=CONFIG['data'].get(\"num_workers\"),\n",
    "    encoder='gpt2'\n",
    ")\n",
    "\n",
    "dm.prepare_data()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NanoGPT Model\n",
    "model = NanoGPT(\n",
    "    d_model=CONFIG[\"model\"].get(\"d_model\"),\n",
    "    seq_len=CONFIG[\"data\"].get(\"seq_len\"),\n",
    "    vocab_size=dm.train_ds.vocab_size,\n",
    "    n_head=CONFIG[\"model\"].get(\"n_head\"),\n",
    "    n_layer=CONFIG[\"model\"].get(\"n_layer\"),\n",
    "    lr=CONFIG[\"lr\"],\n",
    "    bias=False,\n",
    "    dropout_rate=CONFIG[\"model\"].get(\"dropout\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59733/384950585.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  CHKPOINT = torch.load(os.path.join(os.getcwd(),\"logs\",\"chkpoints\",'epoch=04.ckpt'))\n"
     ]
    }
   ],
   "source": [
    "# CHKPOINT = torch.load(os.path.join(os.getcwd(),\"logs\",\"chkpoints\",'nanogpt_epoch=01.ckpt'))\n",
    "CHKPOINT = torch.load(os.path.join(os.getcwd(),\"logs\",\"chkpoints\",'epoch=04.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(CHKPOINT['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(dm.train_dataloader()))\n",
    "# ip,op = batch\n",
    "# logger.log_graph(model.to(DEVICE),(ip.to(DEVICE),op.to(DEVICE)))\n",
    "# with torch.autograd.profiler.profile() as prof:\n",
    "#     output = model.to(DEVICE)(batch[0].to(DEVICE))\n",
    "\n",
    "# os.makedirs(name=os.path.join(os.path.dirname(__file__),'logs','profiler'),exist_ok=True)\n",
    "# with open(os.path.join(os.path.dirname(__file__),'logs','profiler',\"cpu_throttle.txt\"), \"w\") as text_file:\n",
    "#     text_file.write(f\"{prof.key_averages().table(sort_by='self_cpu_time_total',top_level_events_only=False)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Summary\n",
    "# summary(\n",
    "#     model=model,\n",
    "#     input_data=batch[0],\n",
    "#     depth=5,\n",
    "#     verbose=2,\n",
    "#     col_width=16,\n",
    "#     col_names=[\n",
    "#         \"input_size\",\n",
    "#         \"output_size\",\n",
    "#         \"num_params\",\n",
    "#         \"kernel_size\",\n",
    "#         \"mult_adds\",\n",
    "#     ],\n",
    "#     row_settings=[\"var_names\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "/home/muthu/miniconda3/envs/venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/amp.py:52: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=CONFIG[\"trainer\"].get(\"epoch\"),\n",
    "    # callbacks=call_backs,\n",
    "    # logger=logger,\n",
    "    precision='16-mixed',\n",
    "    profiler='pytorch',#perf_profiler,#'advanced',\n",
    "    enable_model_summary=False,\n",
    "    enable_progress_bar=False,\n",
    "    enable_checkpointing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /home/muthu/GitHub/spaces ğŸš€/NanoTalker/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/muthu/miniconda3/envs/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "[W723 23:07:19.762330026 collection.cpp:613] Warning: void cutlass::Kernel2<cutlass_75_tensorop_f16_s1688gemm_f16_128x64_tn_align1>(cutlass_75_tensorop_f16_s1688gemm_f16_128x64_tn_align1::Params) (function operator())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">      Validate metric      </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">      val/loss_epoch       </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">     2.617523193359375     </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m     val/loss_epoch      \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m    2.617523193359375    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALIDATE Profiler Report\n",
      "Profile stats for: records\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          ProfilerStep*         2.13%       3.060ms        82.48%     118.530ms      39.510ms       0.000us         0.00%      79.126ms      26.375ms             3  \n",
      "                                             aten::item         0.02%      25.767us        47.09%      67.677ms       7.520ms       0.000us         0.00%      13.057us       1.451us             9  \n",
      "                              aten::_local_scalar_dense         0.06%      90.751us        47.07%      67.651ms       7.517ms      13.057us         0.01%      13.057us       1.451us             9  \n",
      "                                  cudaStreamSynchronize        46.95%      67.479ms        46.95%      67.479ms       7.498ms       0.000us         0.00%       0.000us       0.000us             9  \n",
      "[pl][profile][Strategy]SingleDeviceStrategy.validati...         1.07%       1.537ms        30.92%      44.437ms      14.812ms       0.000us         0.00%      79.101ms      26.367ms             3  \n",
      "[pl][module]torch.nn.modules.container.Sequential: d...         0.19%     275.013us        27.92%      40.127ms      13.376ms       0.000us         0.00%       3.771ms       1.257ms             3  \n",
      "                                  cudaDeviceSynchronize        17.52%      25.182ms        17.52%      25.182ms      25.182ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "      [pl][module]model.decoder.DecoderBlock: decoder.0         0.22%     316.689us         7.15%      10.270ms       3.423ms       0.000us         0.00%     945.470us     315.157us             3  \n",
      "                                           aten::matmul         0.76%       1.088ms         7.14%      10.267ms      45.032us       0.000us         0.00%       1.346ms       5.905us           228  \n",
      "      [pl][module]model.decoder.DecoderBlock: decoder.2         0.24%     342.687us         7.01%      10.067ms       3.356ms       0.000us         0.00%     971.329us     323.776us             3  \n",
      "                                           aten::linear         0.37%     534.669us         6.95%       9.989ms      57.410us       0.000us         0.00%      22.855ms     131.349us           174  \n",
      "      [pl][module]model.decoder.DecoderBlock: decoder.3         0.23%     323.478us         6.81%       9.792ms       3.264ms       0.000us         0.00%     929.612us     309.871us             3  \n",
      "      [pl][module]model.decoder.DecoderBlock: decoder.1         0.23%     331.867us         6.76%       9.722ms       3.241ms       0.000us         0.00%     924.319us     308.106us             3  \n",
      "[pl][module]model.decoder.MultiHeadAttention: decode...         0.29%     415.394us         5.17%       7.423ms       2.474ms       0.000us         0.00%     539.043us     179.681us             3  \n",
      "[pl][module]model.decoder.MultiHeadAttention: decode...         0.28%     398.535us         4.88%       7.010ms       2.337ms       0.000us         0.00%     535.688us     178.563us             3  \n",
      "[pl][module]model.decoder.MultiHeadAttention: decode...         0.28%     400.717us         4.81%       6.909ms       2.303ms       0.000us         0.00%     526.621us     175.540us             3  \n",
      "[pl][module]model.decoder.MultiHeadAttention: decode...         0.28%     395.979us         4.73%       6.799ms       2.266ms       0.000us         0.00%     561.440us     187.147us             3  \n",
      "                                       cudaLaunchKernel         2.91%       4.185ms         2.91%       4.185ms       6.091us       0.000us         0.00%       0.000us       0.000us           687  \n",
      "                                               aten::to         0.11%     155.083us         2.64%       3.790ms      18.046us       0.000us         0.00%       1.094ms       5.210us           210  \n",
      "                                         aten::_to_copy         0.27%     384.812us         2.53%       3.635ms      18.083us       0.000us         0.00%       1.094ms       5.443us           201  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 143.712ms\n",
      "Self CUDA time total: 202.609ms\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val/loss_epoch': 2.617523193359375}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validating\n",
    "trainer.validate(model, dataloaders=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generation\n",
    "def generate_text(prompt: str, max_new_token: int, dm=dm, imodel=model,device=DEVICE):\n",
    "    encoded = dm.train_ds.enc.encode_ordinary(prompt)\n",
    "    imodel.to(device)\n",
    "    with torch.no_grad():\n",
    "        encoded_text = torch.tensor(encoded,device=device).unsqueeze(0)\n",
    "        new_word_predict = []\n",
    "        for _ in range(max_new_token):\n",
    "            encoded_text = encoded_text[:, -32:]\n",
    "            logits, _ = imodel(encoded_text)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            next_word = torch.multinomial(probs, num_samples=1)\n",
    "            new_word_predict.append(next_word.item())\n",
    "            encoded_text = torch.cat((encoded_text[:, imodel.seq_len:], next_word), dim=1)             # Beam Search in Decodiing Strategy\n",
    "    res = encoded + new_word_predict\n",
    "    return dm.train_ds.decode(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let us loves himury trpot Perever distraught wide lookingopen prosperity imitateuf occ\n",
      "\n",
      "OrRush autumn infectious spotethetition book arroganceChKE divorced Bad fond statue\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"Let us loves him\",30,dm=dm,imodel=model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c69739d2ff79f8d9700f86de6b3b86127b80ed5c02d8f9463f7fe7d202dc3ed5"
  },
  "kernelspec": {
   "display_name": "Python 3.10.14 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
