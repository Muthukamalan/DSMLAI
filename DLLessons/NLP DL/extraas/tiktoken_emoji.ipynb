{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziirFV9PaG-T",
        "outputId": "a520fd55-e9db-4b05-8ed0-677ce29ec95f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.12.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken emoji --no-cache-dir"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init the GPT-4 Tokenizer\n",
        "import tiktoken\n",
        "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "print(enc.n_vocab) # number of tokens in total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57wUOMOhaL2y",
        "outputId": "2653a54d-09cd-48d6-c6b6-26d14fdc10ea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init the emojis\n",
        "import emoji\n",
        "emojis = list(emoji.EMOJI_DATA.keys())\n",
        "import random\n",
        "random.seed(15)\n",
        "random.shuffle(emojis)\n",
        "print(len(emoji.EMOJI_DATA)) # number of possible emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IODCjlnLeNjh",
        "outputId": "ec1487ea-1359-4573-8ad0-92a803f4f724"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(emojis))\n",
        "print(emojis[:10]+ emojis[-10:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSms8_ul2qlp",
        "outputId": "7951c685-1196-4354-b2f5-a3aed0e6a7de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5034\n",
            "['🧎\\u200d♀', '📏', '🤾🏻\\u200d♀️', '🧑🏾\\u200d🦼\\u200d➡️', '🍏', '🙍\\u200d♀️', '✌🏿', '🤙🏻', '🈴', '🧑\\u200d🦼\\u200d➡️', '🦜', '🧯', '🍌', '🇯🇲', '👷🏼\\u200d♂️', '👧', '🇵🇲', '🤦\\u200d♀', '🇪🇹', '👩🏿\\u200d❤️\\u200d💋\\u200d👩🏼']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_tokens(text, max_per_row=10):\n",
        "    ids = enc.encode(text)\n",
        "    unique_tokens = set(ids)\n",
        "\n",
        "    # map all tokens we see to a unique emoji\n",
        "    id_to_emoji = {id: emoji for emoji, id in zip(emojis, unique_tokens)}\n",
        "\n",
        "\n",
        "    # do the translatation\n",
        "    lines = []\n",
        "    for i in range(0, len(ids), max_per_row):\n",
        "        lines.append(''.join([id_to_emoji[id] for id in ids[i:i+max_per_row]]))\n",
        "    out = '\\n'.join(lines)\n",
        "    return out"
      ],
      "metadata": {
        "id": "KvSEuUZreWms"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Words vs Tokens\n",
        "A word is most likely what you think it is - the most simple form or unit of language as understood by humans. In the sentence, “I like cats”, there are three words - “I”, “like”, and “cats.” We can think of words as the primary building blocks of language; the fundamental pieces of language that we are taught from a very young age.\n",
        "\n",
        "A token is a bit more complex. Tokenization is the process of converting pieces of language into bits of data that are usable for a program, and a tokenizer is an algorithm or function that performs this process, i.e., takes language and converts it into these usable bits of data. Thus, a token is a unit of text that is intentionally segmented for a large language model to process efficiently. These units can be words or any other subset of language - parts of words, combinations of words, or punctuation.\n",
        "\n",
        "There are a variety of different tokenizers out there which reflect a variety of trade offs. Well-known tokenizers include NLTK (Natural Language Toolkit), Spacy, BERT tokenizer and Keras. Whether or not to select one of these or a different tokenizer depends upon your specific use case. On average, there are roughly 0.75 words per token, but there can be meaningful differences among tokenizers.\"\"\"\n",
        "\n",
        "print(text_to_tokens(text, max_per_row=15))\n",
        "print(enc.encode(text))\n",
        "print(len(enc.encode(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjCMId2KaPWG",
        "outputId": "bd3075ed-a7c2-4f26-8398-c3fc6493383c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧍🏿🏃🏾‍➡👩🏻‍🤝‍👩🏼🕴🏾🏋🏿🚶🏼‍♂️‍➡🏋🏾‍♀🇼🇸🤝🏾❎🫷👩🏾‍🦯🖖🏾🏋🏾‍♀👨🏻‍🦲\n",
            "🏌‍♂️🇼🇸👳‍♂️🧑🏽‍❤‍🧑🏾🫁🧑🏽‍🤝‍🧑🏽🎗️🔓👱‍♀👩🏻‍❤️‍💋‍👨🏾🪱🧵🤾🏻‍♀️🧑🏽‍⚖🏌‍♂️\n",
            "🔣🧎‍♀🧙🏽‍♀️👮🏾🇪🇷🤙🏻🧑🏼‍❤️‍💋‍🧑🏾🤎🍏👨‍👩‍👦🧑🏼‍🏫👨🏻‍🦲🧙🏽‍♀️👮🏾🧑🏼‍❤️‍💋‍🧑🏾\n",
            "🧙🏽‍♀️🧑🏾‍🦯‍➡🧑🏼‍❤️‍💋‍🧑🏾👨🏾‍🦲🧙🏽‍♀️🍱🧚🏻‍♀️👩🏻‍❤‍💋‍👩🏾🇳🇫👩🏾‍🦯🎗️🧑🏼‍🏫👱‍♀🏌‍♂️📏\n",
            "👩🏻‍🦲↕️🎗️🔓🧑‍🦼‍➡️🏌‍♂️👨🏿‍❤️‍💋‍👨🏻👩🏿‍❤️‍👩🏻🎗️🔓🇷🇪🇨🇦🍏👁️‍🗨️🏃🏽‍♂️‍➡\n",
            "🖲☪️👱🏾‍♂🚶🏾‍♀‍➡🤫🏋🏿🇬🇹🏋🏾‍♀🖲🌫🧑🏻‍🦯‍➡️🎅🏼🤾🏻‍♀️🏄🏾‍♂✌🏿\n",
            "🏋🏾‍♀🏌‍♂️🔃🎗️🧑🏾‍❤‍💋‍🧑🏿👩🏿‍❤️‍👩🏻🎗️🔓🥘🧑🏻‍🎓🎗️📮🇷🇪🍏🕵🏼‍♀️\n",
            "☺🖲🈴🧎‍♀👨🏾‍🦲🖲📕🏋🏾‍♀🌶🫱🫁🇫🇷🇷🇪😕👩‍🦽‍➡\n",
            "🔃🧎‍♀🦸🏾‍♂️🧑🏽‍⚕🤸🏾‍♂⏰🔓👨🏾‍🦲🧔🏼🖖🏾🥘🏊🕵🏼‍♀️🧑🏻‍🎓🎗️\n",
            "📮🤾🏻‍♀️💇🏾🧎‍♀🖲🇬🇹🏋🏾‍♀🖲🧑🏽‍🤝‍🧑🏽🎗️🚶🏻‍♀‍➡🇷🇪🏋🏾‍♀🧔🏽‍♀👩🏿‍❤️‍💋‍👨🏻\n",
            "☺🖲🏌🔓🧎🏾‍♀‍➡🇰🇾🔃〰️🤾🏻‍♀️👨🏽‍❤‍👨🏽🗜️🇳🇫🕓🧑🏼‍🏫🫁\n",
            "🥖🏄🏼💆🏽‍♀️🎗️🔓👨🏻‍🦲🤲🏿🎗️🧑🏼‍🏫🧎‍♀♀️🎗️🧑🏼‍🏫🧎‍♀🫁\n",
            "🧑‍🍼🤫🇦🇫🍏🖲🧑🏾‍🦼‍➡️🎗️🌭🇬🇹♑🔶🤎🛸🧑🏼‍🦽‍➡️🖲\n",
            "🧑🏾‍🦼‍➡️🎗️🤾‍♀🌱🤾🏻‍♀️👔🧗🏼‍♀️🇬🇹♑🇵🇾🇹🇯👩🏿‍🎓📚🙋🏻‍♀🏾\n",
            "🙇🏿‍♂️📆🧚🏿‍♂✋🏼🧎‍♀👨🏽‍🌾🤸🏼‍♀️📕👨🏾‍🦲👨‍🦽‍➡️🧗🏾‍♂🤾🏻‍♀️🇳🇨🫁🧑‍🦽\n",
            "🇰🇾⏳🤵🏼‍♀️🎗️🏊🫁🖲🌭📕👨‍🍼🗣👩🏿‍🦯🪖🏃🏿‍♀‍➡️🧗🏻‍♀\n",
            "🤾🏻‍♀️🖊🦹🏼‍♂️🧎‍♀🤎🍏🚶🏿‍♂️🤵‍♀🙍‍♀️🤾🏻‍♀️💂🧑🏼‍🏫🪤🇬🇹🧎‍♀\n",
            "💌🤎🇳🇫🕓⬆️👩🏾‍🦳🙅‍♀🇬🇹♑🤾🏻‍♀️\n",
            "[24390, 6296, 59266, 198, 32, 3492, 374, 1455, 4461, 1148, 499, 1781, 433, 374, 482, 279, 1455, 4382, 1376, 477, 5089, 315, 4221, 439, 16365, 555, 12966, 13, 763, 279, 11914, 11, 1054, 40, 1093, 19987, 9520, 1070, 527, 2380, 4339, 482, 1054, 40, 9520, 1054, 4908, 9520, 323, 1054, 38552, 2029, 1226, 649, 1781, 315, 4339, 439, 279, 6156, 4857, 10215, 315, 4221, 26, 279, 16188, 9863, 315, 4221, 430, 584, 527, 15972, 505, 264, 1633, 3995, 4325, 382, 32, 4037, 374, 264, 2766, 810, 6485, 13, 9857, 2065, 374, 279, 1920, 315, 34537, 9863, 315, 4221, 1139, 9660, 315, 828, 430, 527, 41030, 369, 264, 2068, 11, 323, 264, 47058, 374, 459, 12384, 477, 734, 430, 27772, 420, 1920, 11, 602, 1770, 2637, 5097, 4221, 323, 33822, 433, 1139, 1521, 41030, 9660, 315, 828, 13, 14636, 11, 264, 4037, 374, 264, 5089, 315, 1495, 430, 374, 37304, 86045, 369, 264, 3544, 4221, 1646, 311, 1920, 30820, 13, 4314, 8316, 649, 387, 4339, 477, 904, 1023, 27084, 315, 4221, 482, 5596, 315, 4339, 11, 28559, 315, 4339, 11, 477, 62603, 382, 3947, 527, 264, 8205, 315, 2204, 4037, 12509, 704, 1070, 902, 8881, 264, 8205, 315, 6696, 74317, 13, 8489, 22015, 4037, 12509, 2997, 33260, 16155, 320, 55381, 11688, 55876, 705, 3165, 2826, 11, 426, 3481, 47058, 323, 735, 9431, 13, 13440, 477, 539, 311, 3373, 832, 315, 1521, 477, 264, 2204, 47058, 14117, 5304, 701, 3230, 1005, 1162, 13, 1952, 5578, 11, 1070, 527, 17715, 220, 15, 13, 2075, 4339, 824, 4037, 11, 719, 1070, 649, 387, 23222, 12062, 4315, 4037, 12509, 13]\n",
            "265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"How many letters 'r' in the word 'strawberry'?\"\"\"\n",
        "print(text_to_tokens(text, max_per_row=20))\n",
        "print(len(enc.encode(text)))\n",
        "print(enc.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75OlT3yhf9p5",
        "outputId": "48598875-2f41-4694-baff-f0535eb75ddf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧑‍🦽👩🏿‍❤️‍💋‍👨🏻🤾🏻‍♀️🙍‍♀️🤙🏻🧑🏾‍🦼‍➡️✌🏿💂📏🙍‍♀️🈴🧎‍♀🍏🧑‍🦼‍➡️\n",
            "14\n",
            "[4438, 1690, 12197, 364, 81, 6, 304, 279, 3492, 364, 496, 675, 15717, 71090]\n"
          ]
        }
      ]
    }
  ]
}