{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziirFV9PaG-T",
        "outputId": "a520fd55-e9db-4b05-8ed0-677ce29ec95f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.12.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken emoji --no-cache-dir"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init the GPT-4 Tokenizer\n",
        "import tiktoken\n",
        "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "print(enc.n_vocab) # number of tokens in total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57wUOMOhaL2y",
        "outputId": "2653a54d-09cd-48d6-c6b6-26d14fdc10ea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init the emojis\n",
        "import emoji\n",
        "emojis = list(emoji.EMOJI_DATA.keys())\n",
        "import random\n",
        "random.seed(15)\n",
        "random.shuffle(emojis)\n",
        "print(len(emoji.EMOJI_DATA)) # number of possible emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IODCjlnLeNjh",
        "outputId": "ec1487ea-1359-4573-8ad0-92a803f4f724"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(emojis))\n",
        "print(emojis[:10]+ emojis[-10:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSms8_ul2qlp",
        "outputId": "7951c685-1196-4354-b2f5-a3aed0e6a7de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5034\n",
            "['ğŸ§\\u200dâ™€', 'ğŸ“', 'ğŸ¤¾ğŸ»\\u200dâ™€ï¸', 'ğŸ§‘ğŸ¾\\u200dğŸ¦¼\\u200dâ¡ï¸', 'ğŸ', 'ğŸ™\\u200dâ™€ï¸', 'âœŒğŸ¿', 'ğŸ¤™ğŸ»', 'ğŸˆ´', 'ğŸ§‘\\u200dğŸ¦¼\\u200dâ¡ï¸', 'ğŸ¦œ', 'ğŸ§¯', 'ğŸŒ', 'ğŸ‡¯ğŸ‡²', 'ğŸ‘·ğŸ¼\\u200dâ™‚ï¸', 'ğŸ‘§', 'ğŸ‡µğŸ‡²', 'ğŸ¤¦\\u200dâ™€', 'ğŸ‡ªğŸ‡¹', 'ğŸ‘©ğŸ¿\\u200dâ¤ï¸\\u200dğŸ’‹\\u200dğŸ‘©ğŸ¼']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_tokens(text, max_per_row=10):\n",
        "    ids = enc.encode(text)\n",
        "    unique_tokens = set(ids)\n",
        "\n",
        "    # map all tokens we see to a unique emoji\n",
        "    id_to_emoji = {id: emoji for emoji, id in zip(emojis, unique_tokens)}\n",
        "\n",
        "\n",
        "    # do the translatation\n",
        "    lines = []\n",
        "    for i in range(0, len(ids), max_per_row):\n",
        "        lines.append(''.join([id_to_emoji[id] for id in ids[i:i+max_per_row]]))\n",
        "    out = '\\n'.join(lines)\n",
        "    return out"
      ],
      "metadata": {
        "id": "KvSEuUZreWms"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Words vs Tokens\n",
        "A word is most likely what you think it is - the most simple form or unit of language as understood by humans. In the sentence, â€œI like catsâ€, there are three words - â€œIâ€, â€œlikeâ€, and â€œcats.â€ We can think of words as the primary building blocks of language; the fundamental pieces of language that we are taught from a very young age.\n",
        "\n",
        "A token is a bit more complex. Tokenization is the process of converting pieces of language into bits of data that are usable for a program, and a tokenizer is an algorithm or function that performs this process, i.e., takes language and converts it into these usable bits of data. Thus, a token is a unit of text that is intentionally segmented for a large language model to process efficiently. These units can be words or any other subset of language - parts of words, combinations of words, or punctuation.\n",
        "\n",
        "There are a variety of different tokenizers out there which reflect a variety of trade offs. Well-known tokenizers include NLTK (Natural Language Toolkit), Spacy, BERT tokenizer and Keras. Whether or not to select one of these or a different tokenizer depends upon your specific use case. On average, there are roughly 0.75 words per token, but there can be meaningful differences among tokenizers.\"\"\"\n",
        "\n",
        "print(text_to_tokens(text, max_per_row=15))\n",
        "print(enc.encode(text))\n",
        "print(len(enc.encode(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjCMId2KaPWG",
        "outputId": "bd3075ed-a7c2-4f26-8398-c3fc6493383c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§ğŸ¿ğŸƒğŸ¾â€â¡ğŸ‘©ğŸ»â€ğŸ¤â€ğŸ‘©ğŸ¼ğŸ•´ğŸ¾ğŸ‹ğŸ¿ğŸš¶ğŸ¼â€â™‚ï¸â€â¡ğŸ‹ğŸ¾â€â™€ğŸ‡¼ğŸ‡¸ğŸ¤ğŸ¾âğŸ«·ğŸ‘©ğŸ¾â€ğŸ¦¯ğŸ––ğŸ¾ğŸ‹ğŸ¾â€â™€ğŸ‘¨ğŸ»â€ğŸ¦²\n",
            "ğŸŒâ€â™‚ï¸ğŸ‡¼ğŸ‡¸ğŸ‘³â€â™‚ï¸ğŸ§‘ğŸ½â€â¤â€ğŸ§‘ğŸ¾ğŸ«ğŸ§‘ğŸ½â€ğŸ¤â€ğŸ§‘ğŸ½ğŸ—ï¸ğŸ”“ğŸ‘±â€â™€ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ğŸª±ğŸ§µğŸ¤¾ğŸ»â€â™€ï¸ğŸ§‘ğŸ½â€âš–ğŸŒâ€â™‚ï¸\n",
            "ğŸ”£ğŸ§â€â™€ğŸ§™ğŸ½â€â™€ï¸ğŸ‘®ğŸ¾ğŸ‡ªğŸ‡·ğŸ¤™ğŸ»ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¾ğŸ¤ğŸğŸ‘¨â€ğŸ‘©â€ğŸ‘¦ğŸ§‘ğŸ¼â€ğŸ«ğŸ‘¨ğŸ»â€ğŸ¦²ğŸ§™ğŸ½â€â™€ï¸ğŸ‘®ğŸ¾ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¾\n",
            "ğŸ§™ğŸ½â€â™€ï¸ğŸ§‘ğŸ¾â€ğŸ¦¯â€â¡ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¾ğŸ‘¨ğŸ¾â€ğŸ¦²ğŸ§™ğŸ½â€â™€ï¸ğŸ±ğŸ§šğŸ»â€â™€ï¸ğŸ‘©ğŸ»â€â¤â€ğŸ’‹â€ğŸ‘©ğŸ¾ğŸ‡³ğŸ‡«ğŸ‘©ğŸ¾â€ğŸ¦¯ğŸ—ï¸ğŸ§‘ğŸ¼â€ğŸ«ğŸ‘±â€â™€ğŸŒâ€â™‚ï¸ğŸ“\n",
            "ğŸ‘©ğŸ»â€ğŸ¦²â†•ï¸ğŸ—ï¸ğŸ”“ğŸ§‘â€ğŸ¦¼â€â¡ï¸ğŸŒâ€â™‚ï¸ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ»ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘©ğŸ»ğŸ—ï¸ğŸ”“ğŸ‡·ğŸ‡ªğŸ‡¨ğŸ‡¦ğŸğŸ‘ï¸â€ğŸ—¨ï¸ğŸƒğŸ½â€â™‚ï¸â€â¡\n",
            "ğŸ–²â˜ªï¸ğŸ‘±ğŸ¾â€â™‚ğŸš¶ğŸ¾â€â™€â€â¡ğŸ¤«ğŸ‹ğŸ¿ğŸ‡¬ğŸ‡¹ğŸ‹ğŸ¾â€â™€ğŸ–²ğŸŒ«ğŸ§‘ğŸ»â€ğŸ¦¯â€â¡ï¸ğŸ…ğŸ¼ğŸ¤¾ğŸ»â€â™€ï¸ğŸ„ğŸ¾â€â™‚âœŒğŸ¿\n",
            "ğŸ‹ğŸ¾â€â™€ğŸŒâ€â™‚ï¸ğŸ”ƒğŸ—ï¸ğŸ§‘ğŸ¾â€â¤â€ğŸ’‹â€ğŸ§‘ğŸ¿ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘©ğŸ»ğŸ—ï¸ğŸ”“ğŸ¥˜ğŸ§‘ğŸ»â€ğŸ“ğŸ—ï¸ğŸ“®ğŸ‡·ğŸ‡ªğŸğŸ•µğŸ¼â€â™€ï¸\n",
            "â˜ºğŸ–²ğŸˆ´ğŸ§â€â™€ğŸ‘¨ğŸ¾â€ğŸ¦²ğŸ–²ğŸ“•ğŸ‹ğŸ¾â€â™€ğŸŒ¶ğŸ«±ğŸ«ğŸ‡«ğŸ‡·ğŸ‡·ğŸ‡ªğŸ˜•ğŸ‘©â€ğŸ¦½â€â¡\n",
            "ğŸ”ƒğŸ§â€â™€ğŸ¦¸ğŸ¾â€â™‚ï¸ğŸ§‘ğŸ½â€âš•ğŸ¤¸ğŸ¾â€â™‚â°ğŸ”“ğŸ‘¨ğŸ¾â€ğŸ¦²ğŸ§”ğŸ¼ğŸ––ğŸ¾ğŸ¥˜ğŸŠğŸ•µğŸ¼â€â™€ï¸ğŸ§‘ğŸ»â€ğŸ“ğŸ—ï¸\n",
            "ğŸ“®ğŸ¤¾ğŸ»â€â™€ï¸ğŸ’‡ğŸ¾ğŸ§â€â™€ğŸ–²ğŸ‡¬ğŸ‡¹ğŸ‹ğŸ¾â€â™€ğŸ–²ğŸ§‘ğŸ½â€ğŸ¤â€ğŸ§‘ğŸ½ğŸ—ï¸ğŸš¶ğŸ»â€â™€â€â¡ğŸ‡·ğŸ‡ªğŸ‹ğŸ¾â€â™€ğŸ§”ğŸ½â€â™€ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ»\n",
            "â˜ºğŸ–²ğŸŒğŸ”“ğŸ§ğŸ¾â€â™€â€â¡ğŸ‡°ğŸ‡¾ğŸ”ƒã€°ï¸ğŸ¤¾ğŸ»â€â™€ï¸ğŸ‘¨ğŸ½â€â¤â€ğŸ‘¨ğŸ½ğŸ—œï¸ğŸ‡³ğŸ‡«ğŸ•“ğŸ§‘ğŸ¼â€ğŸ«ğŸ«\n",
            "ğŸ¥–ğŸ„ğŸ¼ğŸ’†ğŸ½â€â™€ï¸ğŸ—ï¸ğŸ”“ğŸ‘¨ğŸ»â€ğŸ¦²ğŸ¤²ğŸ¿ğŸ—ï¸ğŸ§‘ğŸ¼â€ğŸ«ğŸ§â€â™€â™€ï¸ğŸ—ï¸ğŸ§‘ğŸ¼â€ğŸ«ğŸ§â€â™€ğŸ«\n",
            "ğŸ§‘â€ğŸ¼ğŸ¤«ğŸ‡¦ğŸ‡«ğŸğŸ–²ğŸ§‘ğŸ¾â€ğŸ¦¼â€â¡ï¸ğŸ—ï¸ğŸŒ­ğŸ‡¬ğŸ‡¹â™‘ğŸ”¶ğŸ¤ğŸ›¸ğŸ§‘ğŸ¼â€ğŸ¦½â€â¡ï¸ğŸ–²\n",
            "ğŸ§‘ğŸ¾â€ğŸ¦¼â€â¡ï¸ğŸ—ï¸ğŸ¤¾â€â™€ğŸŒ±ğŸ¤¾ğŸ»â€â™€ï¸ğŸ‘”ğŸ§—ğŸ¼â€â™€ï¸ğŸ‡¬ğŸ‡¹â™‘ğŸ‡µğŸ‡¾ğŸ‡¹ğŸ‡¯ğŸ‘©ğŸ¿â€ğŸ“ğŸ“šğŸ™‹ğŸ»â€â™€ğŸ¾\n",
            "ğŸ™‡ğŸ¿â€â™‚ï¸ğŸ“†ğŸ§šğŸ¿â€â™‚âœ‹ğŸ¼ğŸ§â€â™€ğŸ‘¨ğŸ½â€ğŸŒ¾ğŸ¤¸ğŸ¼â€â™€ï¸ğŸ“•ğŸ‘¨ğŸ¾â€ğŸ¦²ğŸ‘¨â€ğŸ¦½â€â¡ï¸ğŸ§—ğŸ¾â€â™‚ğŸ¤¾ğŸ»â€â™€ï¸ğŸ‡³ğŸ‡¨ğŸ«ğŸ§‘â€ğŸ¦½\n",
            "ğŸ‡°ğŸ‡¾â³ğŸ¤µğŸ¼â€â™€ï¸ğŸ—ï¸ğŸŠğŸ«ğŸ–²ğŸŒ­ğŸ“•ğŸ‘¨â€ğŸ¼ğŸ—£ğŸ‘©ğŸ¿â€ğŸ¦¯ğŸª–ğŸƒğŸ¿â€â™€â€â¡ï¸ğŸ§—ğŸ»â€â™€\n",
            "ğŸ¤¾ğŸ»â€â™€ï¸ğŸ–ŠğŸ¦¹ğŸ¼â€â™‚ï¸ğŸ§â€â™€ğŸ¤ğŸğŸš¶ğŸ¿â€â™‚ï¸ğŸ¤µâ€â™€ğŸ™â€â™€ï¸ğŸ¤¾ğŸ»â€â™€ï¸ğŸ’‚ğŸ§‘ğŸ¼â€ğŸ«ğŸª¤ğŸ‡¬ğŸ‡¹ğŸ§â€â™€\n",
            "ğŸ’ŒğŸ¤ğŸ‡³ğŸ‡«ğŸ•“â¬†ï¸ğŸ‘©ğŸ¾â€ğŸ¦³ğŸ™…â€â™€ğŸ‡¬ğŸ‡¹â™‘ğŸ¤¾ğŸ»â€â™€ï¸\n",
            "[24390, 6296, 59266, 198, 32, 3492, 374, 1455, 4461, 1148, 499, 1781, 433, 374, 482, 279, 1455, 4382, 1376, 477, 5089, 315, 4221, 439, 16365, 555, 12966, 13, 763, 279, 11914, 11, 1054, 40, 1093, 19987, 9520, 1070, 527, 2380, 4339, 482, 1054, 40, 9520, 1054, 4908, 9520, 323, 1054, 38552, 2029, 1226, 649, 1781, 315, 4339, 439, 279, 6156, 4857, 10215, 315, 4221, 26, 279, 16188, 9863, 315, 4221, 430, 584, 527, 15972, 505, 264, 1633, 3995, 4325, 382, 32, 4037, 374, 264, 2766, 810, 6485, 13, 9857, 2065, 374, 279, 1920, 315, 34537, 9863, 315, 4221, 1139, 9660, 315, 828, 430, 527, 41030, 369, 264, 2068, 11, 323, 264, 47058, 374, 459, 12384, 477, 734, 430, 27772, 420, 1920, 11, 602, 1770, 2637, 5097, 4221, 323, 33822, 433, 1139, 1521, 41030, 9660, 315, 828, 13, 14636, 11, 264, 4037, 374, 264, 5089, 315, 1495, 430, 374, 37304, 86045, 369, 264, 3544, 4221, 1646, 311, 1920, 30820, 13, 4314, 8316, 649, 387, 4339, 477, 904, 1023, 27084, 315, 4221, 482, 5596, 315, 4339, 11, 28559, 315, 4339, 11, 477, 62603, 382, 3947, 527, 264, 8205, 315, 2204, 4037, 12509, 704, 1070, 902, 8881, 264, 8205, 315, 6696, 74317, 13, 8489, 22015, 4037, 12509, 2997, 33260, 16155, 320, 55381, 11688, 55876, 705, 3165, 2826, 11, 426, 3481, 47058, 323, 735, 9431, 13, 13440, 477, 539, 311, 3373, 832, 315, 1521, 477, 264, 2204, 47058, 14117, 5304, 701, 3230, 1005, 1162, 13, 1952, 5578, 11, 1070, 527, 17715, 220, 15, 13, 2075, 4339, 824, 4037, 11, 719, 1070, 649, 387, 23222, 12062, 4315, 4037, 12509, 13]\n",
            "265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"How many letters 'r' in the word 'strawberry'?\"\"\"\n",
        "print(text_to_tokens(text, max_per_row=20))\n",
        "print(len(enc.encode(text)))\n",
        "print(enc.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75OlT3yhf9p5",
        "outputId": "48598875-2f41-4694-baff-f0535eb75ddf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§‘â€ğŸ¦½ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ»ğŸ¤¾ğŸ»â€â™€ï¸ğŸ™â€â™€ï¸ğŸ¤™ğŸ»ğŸ§‘ğŸ¾â€ğŸ¦¼â€â¡ï¸âœŒğŸ¿ğŸ’‚ğŸ“ğŸ™â€â™€ï¸ğŸˆ´ğŸ§â€â™€ğŸğŸ§‘â€ğŸ¦¼â€â¡ï¸\n",
            "14\n",
            "[4438, 1690, 12197, 364, 81, 6, 304, 279, 3492, 364, 496, 675, 15717, 71090]\n"
          ]
        }
      ]
    }
  ]
}